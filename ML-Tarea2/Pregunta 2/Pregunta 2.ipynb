{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) En primer lugar se crea el dataframe y se analizan sus registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones training set:  (3554, 2)\n",
      "Dimensiones testing set:  (3554, 2)\n",
      "----------------------------------------------------------\n",
      "Conteo de registros de cada clase en el training set\n",
      "-1    1784\n",
      " 1    1770\n",
      "Name: Sentiment, dtype: int64\n",
      "----------------------------------------------------------\n",
      "Conteo de registros de cada clase en el testing set\n",
      "-1    1803\n",
      " 1    1751\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "print \"Dimensiones training set: \", train_df.shape\n",
    "print \"Dimensiones testing set: \", test_df.shape\n",
    "print \"----------------------------------------------------------\"\n",
    "print  \"Conteo de registros de cada clase en el training set\"\n",
    "print train_df['Sentiment'].value_counts()\n",
    "print \"----------------------------------------------------------\"\n",
    "print  \"Conteo de registros de cada clase en el testing set\"\n",
    "print test_df['Sentiment'].value_counts()\n",
    "#print train_df.head()\n",
    "#print test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) En esta sección se pre procesa el texto mediante la utilización de las operaciones lower-casing, y stemming. Para ello se crea la función word_extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " dont love eat cake\n",
      " like learn univers\n",
      " cat play fish\n",
      " believ miracl\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def word_extractor(text,stop = 1):\n",
    "    #wordlemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    '''wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "                  for word in word_tokenize(text.decode('utf-8', 'ignore')) ]'''\n",
    "    wordtokens = [ stemmer.stem(word.lower()) \\\n",
    "                  for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if stop==1:\n",
    "            if word not in commonwords:\n",
    "                words+=\" \"+word\n",
    "        else:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I dont love eating cake\")\n",
    "print word_extractor(\"I like learning in  university\")\n",
    "print word_extractor(\"The cats are playing with fishes\")\n",
    "print word_extractor(\"He believes in miracles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso anterior, el stemming lleva todas las palabras a su raíz sin importar el contexto en donde estas se encuentren. Stemming no puede discriminar, y solo se basa en ver las conjugaciones de las palabras de forma heurísticas, y dependiendo de la terminación es como decide a que raíz pertenece. Esto puede ocasionar problemas como por ejemplo con la palabra university, en donde el proceso de stemming la transforma a univers y puede ocasionar perdida de informacion ya que cambia el significado.\n",
    "\n",
    "Si no se realiza stemming, las palabras quedarían conjugadas y para un procesador de texto o clasificador, no serían equivalentes aunque estén en un contexto similar. El problema es entonces que se generarían mas palabras, con lo cual aumenta el vocabulario y por ende la complejidad del dataset.\n",
    "\n",
    "Con respecto a los \"stop words\", estos se definen en un conjunto de articulos, pronombres y preposiciones, y son quitados de las cadenas de texto, ya que son palabras muy comunes y que no aportan significado al texto, aunque en algunas aplicaciones si pueden llegar a ser utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) En este caso se utiliza un lemmatizer para comparar con los resultados anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " dont love eating cake\n",
      " like learning university\n",
      " cat playing fish\n",
      " belief miracle\n"
     ]
    }
   ],
   "source": [
    "def word_extractor2(text, stop=1):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "                  for word in word_tokenize(text.decode('utf-8','ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if stop==1:\n",
    "            if word not in commonwords:\n",
    "                words+=\" \"+word\n",
    "        else:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print word_extractor2(\"I love to eat cake\")\n",
    "print word_extractor2(\"I love eating cake\")\n",
    "print word_extractor2(\"I loved eating the cake\")\n",
    "print word_extractor2(\"I do not love eating cake\")\n",
    "print word_extractor2(\"I dont love eating cake\")\n",
    "print word_extractor2(\"I like learning in  university\")\n",
    "print word_extractor2(\"The cats are playing with fishes\")\n",
    "print word_extractor2(\"He believes in miracles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesando las cadenas mediante el lemmatizer, se conserva muchas veces la conjugación de las palabras según el contexto, y también son llevadas a su tronco léxico. Por ejemplo las palabras loved y eating se conservan, y en este caso university se mantiene igualmente, no así en el proceso de stemming. Entonces la lematización es un proceso mucho más complejo, que no solo revisa cada palabra y su terminación, sino que además observa el contexto para realizar la agrupación de palabras similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension conjunto de entrenamiento:  (3554, 9663)\n",
      "Dimension conjunto de pruebas:  (3554, 9663)\n",
      "--------------------------------------------------------------\n",
      "10 Palabras mas frecuentes \n",
      "\n",
      "Conjunto de entrenamiento     Conjunto de pruebas \n",
      "\n",
      "566  film                     558  film\n",
      "481  movie                    540  movie\n",
      "246  one                      250  one\n",
      "245  like                     238  ha\n",
      "224  ha                       230  like\n",
      "183  make                     197  story\n",
      "176  story                    175  character\n",
      "163  character                165  time\n",
      "145  comedy                   161  make\n",
      "143  even                     134  comedy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Se crean las listas con los textos procesados\n",
    "texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "#Se crea la matriz con la cuenta de los tokens(test y train)\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "print \"Dimension conjunto de entrenamiento: \",features_train.shape\n",
    "print \"Dimension conjunto de pruebas: \", features_test.shape\n",
    "print \"--------------------------------------------------------------\"\n",
    "#label 0 y 1\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0)\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist_tr=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "dist_ts=list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "#for tag, count in zip(vocab, dist):\n",
    "#    print count, tag\n",
    "index_tr= sorted(range(len(dist_tr)), key=lambda k: dist_tr[k], reverse=True )\n",
    "index_ts= sorted(range(len(dist_ts)), key=lambda k: dist_ts[k], reverse=True )\n",
    "#print sorted(dist_tr, reverse=True)[0]\n",
    "print \"10 Palabras mas frecuentes \\n\"\n",
    "print \"{0:<30}{1} \\n\".format(\"Conjunto de entrenamiento\",\"Conjunto de pruebas\")\n",
    "for i in range (0,10):\n",
    "    m1=index_tr[i]\n",
    "    m2=index_ts[i]\n",
    "    print '{0:<5}{1:<25}{2:<5}{3}'.format(dist_tr[m1], vocab[m1], dist_ts[m2], vocab[m2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia en la tabla anterior, que el vocabulario se compone de 9663 palabras o tokens,  y que tanto los set de entrenamiento y prueba contienen 3554 registros o \"reviews\" de usuarios. Las 10 palabras más frecuentes se repiten en gran medida en ambos conjuntos y conservan un orden similar, esto refleja que los datos presentan estructuras similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy %s: %f\"%(text,acc_tr)\n",
    "    print \"Test Accuracy %s: %f\"%(text,acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función $\\textit{classification_report}$  imprime 3 métricas las cuales son precision, recall y F1 score.\n",
    "$\\textit{Precision}$ se define como:\n",
    "\n",
    "$$ precision = \\frac{tp}{tp+fp}$$ \n",
    "\n",
    "Donde tp representa los verdaderos positivos y fp los falsos positivos. Intuitivamente es la habilidad del clasificador para no etiquetar como positivo una muestra que es negativa. Su mejor valor es 1 y su peor valor es 0.\n",
    "\n",
    "Recall score se define como: \n",
    "\n",
    "$$ recall = \\frac{tp}{tp+fp}$$ \n",
    "\n",
    "Donde tp representa los verdaderos positivos y fp los falsos negativos. Recall representa la habilidad del clasificador para encontrar todos los ejemplos positivos. Su mejor valor es 1 y su peor valor es 0.\n",
    "\n",
    "Entonces $\\textit{precision}$ representa la exactitud o calidad del clasificador y $\\textit{recall}$ la medida de completitud o cantidad.\n",
    "\n",
    "Finalmente, F1 es una medida que utiliza las dos métricas anteriores en un promedio ponderado. Su fórmula es: \n",
    "\n",
    "$$ f1 = 2* \\frac{precision * recall}{precision+recall}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Para el caso de Bernoulli naive bayes los features deben tener valor 0 o 1, esto lo hace automáticamente BernoulliNB de la librería sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código crea los textos y features sin stop words y además otro utilizando stemming, para posteriormente ser analizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# El caso comun en el puntop (d) es lemmatizer filtrando stopwords\n",
    "# Se crea training y testing set sin filtrar stopwords mediante lemmatizer.\n",
    "\n",
    "texts_train_noSW = [word_extractor2(text, 0) for text in train_df.Text]\n",
    "texts_test_noSW = [word_extractor2(text,0) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train_noSW))\n",
    "#Se crea la matriz con la cuenta de los tokens(test y train)\n",
    "features_train_noSW = vectorizer.transform(texts_train_noSW)\n",
    "features_test_noSW = vectorizer.transform(texts_test_noSW)\n",
    "\n",
    "# Se crea training y testing set utilizando stemming\n",
    "\n",
    "texts_train_stem = [word_extractor(text) for text in train_df.Text]\n",
    "texts_test_stem = [word_extractor(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train_stem))\n",
    "#Se crea la matriz con la cuenta de los tokens(test y train)\n",
    "features_train_stem = vectorizer.transform(texts_train_stem)\n",
    "#print features_train_stem.shape\n",
    "features_test_stem = vectorizer.transform(texts_test_stem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo con lemmatizer y filtrando stop words Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy BernoulliNB: 0.958638\n",
      "Test Accuracy BernoulliNB: 0.738531\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import random\n",
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla anterior muestra que el desempeño de Bernoulli Naive Bayes no es del todo bueno, ya que presenta una alta exactitud en el set de entrenamiento, pero mucho peor en el de pruebas, dando entender que el modelo esta sobre ajustando los datos. Esto también queda en evidencia con los otros indicadores como precision y recall, de donde la calidad para clasificar no es muy alta, y tampoco es completa. Esto puede deberse principalmente a que no se toma en cuenta la cantidad de palabras o tokens en cada registro, sino que son transformados a 0 o 1, perdiendo así información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo con lemmatizer sin filtrar stop words Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy BernoulliNB: 0.955262\n",
      "Test Accuracy BernoulliNB: 0.748663\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.76      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_noSW=do_NAIVE_BAYES(features_train_noSW,labels_train,features_test_noSW,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí no se presenta gran mejora, el modelo sigue sobre ajustando, pero al no considerarse el número de stop words, sino que solo se interpreta como si aparece o no la palabra, no afecta demasiado a los resultados. Una ventaja es que requiere menos procesamiento de cómputo y se obtiene levemente un mejor resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo con stemmer Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy BernoulliNB: 0.942881\n",
      "Test Accuracy BernoulliNB: 0.747819\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_stem = do_NAIVE_BAYES(features_train_stem,labels_train,features_test_stem,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que el caso anterior, la mejora es muy pequeña, sin embargo al usar stemming el procesamiento de los datos es más rápido ya que no debe interpretar el contexto y además genera muchos menos features que lemmatization, 1700 palabras menos en el vocabulario aproximadamente. Esto muestra que bernoulli no se ve afectado por la cantidad de features en gran medida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analisis exploratorio Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26622002  0.73377998] you wouldn't want to live waydowntown , but it is a hilarious place to visit .\n",
      "\n",
      "[ 0.17491976  0.82508024] what makes it worth watching is quaid's performance .\n",
      "\n",
      "[ 0.55956959  0.44043041] both the crime story and the love story are unusual . but they don't fit well together and neither is well told .\n",
      "\n",
      "[ 0.60586349  0.39413651] this overproduced and generally disappointing effort isn't likely to rouse the rush hour crowd .\n",
      "\n",
      "[ 0.9894078  0.0105922] it's another video movie photographed like a film , with the bad lighting that's often written off as indie film naturalism .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_proba(features_test)\n",
    "#print model.classes_\n",
    "spl = random.sample(xrange(len(test_pred)), 5)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
