{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) En primer lugar se crea el dataframe y se analizan sus registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones training set:  (3554, 2)\n",
      "Dimensiones testing set:  (3554, 2)\n",
      "----------------------------------------------------------\n",
      "Conteo de registros de cada clase en el training set\n",
      "-1    1784\n",
      " 1    1770\n",
      "Name: Sentiment, dtype: int64\n",
      "----------------------------------------------------------\n",
      "Conteo de registros de cada clase en el testing set\n",
      "-1    1803\n",
      " 1    1751\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "print \"Dimensiones training set: \", train_df.shape\n",
    "print \"Dimensiones testing set: \", test_df.shape\n",
    "print \"----------------------------------------------------------\"\n",
    "print  \"Conteo de registros de cada clase en el training set\"\n",
    "print train_df['Sentiment'].value_counts()\n",
    "print \"----------------------------------------------------------\"\n",
    "print  \"Conteo de registros de cada clase en el testing set\"\n",
    "print test_df['Sentiment'].value_counts()\n",
    "#print train_df.head()\n",
    "#print test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) En esta seccion se se preprocesa el texto mediante la utilización de las operaciones lower-casing, y stemming. Para ello se crea lafuncion word_extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " dont love eat cake\n",
      " like learn univers\n",
      " cat play fish\n",
      " believ miracl\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def word_extractor(text):\n",
    "    #wordlemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    '''wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "                  for word in word_tokenize(text.decode('utf-8', 'ignore')) ]'''\n",
    "    wordtokens = [ stemmer.stem(word.lower()) \\\n",
    "                  for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I dont love eating cake\")\n",
    "print word_extractor(\"I like learning in  university\")\n",
    "print word_extractor(\"The cats are playing with fishes\")\n",
    "print word_extractor(\"He believes in miracles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso anterior, el stemming lleva todas las palabras a su raíz sin importar el contexto en donde estas se encuentren. Stemming no puede discriminar, y solo se basa en ver las conjugaciones de las palabras de forma heurísticas, y dependiendo de la terminación es como decide a que raíz pertenece. Esto puede ocasionar problemas como por ejemplo con la palabra university, en donde el proceso de stemming la transforma a univers y puede ocasionar perdida de informacion ya que cambia el significado.\n",
    "\n",
    "Si no se realiza stemming, las palabras quedarían conjugadas y para un procesador de texto o clasificador, no serían equivalentes aunque estén en un contexto similar. El problema es entonces que se generarían mas palabras, con lo cual aumenta el vocabulario y por ende la complejidad del dataset.\n",
    "\n",
    "Con respecto a los \"stop words\", estos se definen en un conjunto de articulos, pronombres y preposiciones, y son quitados de las cadenas de texto, ya que son palabras muy comunes y que no aportan significado al texto, aunque en algunas aplicaciones si pueden llegar a ser utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) En este caso se utiliza un lemmatizer para comparar con los resultados anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " dont love eating cake\n",
      " like learning university\n",
      " cat playing fish\n",
      " belief miracle\n"
     ]
    }
   ],
   "source": [
    "def word_extractor2(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "                  for word in word_tokenize(text.decode('utf-8','ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print word_extractor2(\"I love to eat cake\")\n",
    "print word_extractor2(\"I love eating cake\")\n",
    "print word_extractor2(\"I loved eating the cake\")\n",
    "print word_extractor2(\"I do not love eating cake\")\n",
    "print word_extractor2(\"I dont love eating cake\")\n",
    "print word_extractor2(\"I like learning in  university\")\n",
    "print word_extractor2(\"The cats are playing with fishes\")\n",
    "print word_extractor2(\"He believes in miracles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesando las cadenas mediante el lemmatizer, se conserva muchas veces la conjugacion de las palabras segun el contexto, y tambien son llevadas a su tronco lexico. Por ejemplo las palabras loved y eating se conservan, y en este caso university se mantiene igualmente, no asi en el proceso de stemming. Entonces la lematizacion es un proceso mucho mas complejo, que no solo revisa cada palabra y su terminacion, sino que ademas observa el contexto para realizar la agrupacion de palabras similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1. ...,  1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0)\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
